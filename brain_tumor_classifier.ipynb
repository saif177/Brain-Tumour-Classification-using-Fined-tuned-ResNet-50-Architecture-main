{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"brain_tumor_classifier.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"561d2064430c4da9bd3c1ab7a40ff1ae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f5876312446a4c3f8eacc983c26951e0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_67d278f9e50c47b4aae7c2ad19353577","IPY_MODEL_03db0d2335ab461daa38bb624df06cd3"]}},"f5876312446a4c3f8eacc983c26951e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"67d278f9e50c47b4aae7c2ad19353577":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_906e832f95454f93ac43f0893aa8acbc","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":102502400,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":102502400,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c2b86978266d4625bc8fd6e30db2e661"}},"03db0d2335ab461daa38bb624df06cd3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_be9f0d7efc6245579aa7055a45bc451c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 97.8M/97.8M [02:33&lt;00:00, 668kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_987b15ca0c5a49cc9939ea915f4ba62d"}},"906e832f95454f93ac43f0893aa8acbc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c2b86978266d4625bc8fd6e30db2e661":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"be9f0d7efc6245579aa7055a45bc451c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"987b15ca0c5a49cc9939ea915f4ba62d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"6E-C_1HD4-zz"},"source":["#import packages"]},{"cell_type":"code","metadata":{"id":"ppY0E2oBcXNu"},"source":["import torch,gc\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms, models\n","from torchvision.utils import make_grid\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import time\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report, jaccard_similarity_score\n","from google.colab import drive "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mQ_bNRuloctt"},"source":["Import Google Drive for persistent storage of our training data, neural network model weights and other required files."]},{"cell_type":"code","metadata":{"id":"SMpvVU6Tq2jy","executionInfo":{"status":"ok","timestamp":1604025266051,"user_tz":-330,"elapsed":2524,"user":{"displayName":"Hemant Kumar","photoUrl":"","userId":"05930409951288419175"}},"outputId":"8962b978-f938-4b5f-e7bd-aacdbd294075","colab":{"base_uri":"https://localhost:8080/"}},"source":["drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jpckgx9ZE3PR"},"source":["os.chdir(\"/content/drive/My Drive/Final Year Project\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"go-hqw0wokrN"},"source":["Empty GPU's memory/cache for training so we'd clear garbage values in it and more memory will be available"]},{"cell_type":"code","metadata":{"id":"XOjZidblq5JL"},"source":["torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjPKpYJTooMG"},"source":["## Custom Dataset Class\n","\n","Create a custom dataset class that augments each image into 4 different angles: 0, 45, 90, 120, 180, 270, 300, 330 degrees. Fuse it with Pytorch's DataLoader class so data can be loaded, augmented and trained in realtime instead of caching all training samples in memory for augmenting."]},{"cell_type":"code","metadata":{"id":"V4PuME0qmFvZ"},"source":["class BrainTumorDataset(Dataset):\n","  def __init__(self, images, labels):\n","    # images\n","    self.X = images\n","    # labels\n","    self.y = labels\n","    \n","    # Transformation for converting original image array to an image and then convert it to a tensor\n","    self.transform = transforms.Compose([transforms.ToPILImage(),\n","        transforms.ToTensor()\n","    ])\n","\n","    # Transformation for converting original image array to an image, rotate it randomly between -45 degrees and 45 degrees, and then convert it to a tensor\n","    self.transform1 = transforms.Compose([\n","        transforms.ToPILImage(),                                          \n","        transforms.RandomRotation(45),\n","        transforms.ToTensor()                                 \n","    ])\n","\n","    # Transformation for converting original image array to an image, rotate it randomly between -90 degrees and 90 degrees, and then convert it to a tensor\n","    self.transform2 = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.RandomRotation(90),\n","        transforms.ToTensor()                                  \n","    ])\n","\n","    # Transformation for converting original image array to an image, rotate it randomly between -120 degrees and 120 degrees, and then convert it to a tensor\n","    self.transform3 = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.RandomRotation(120),\n","        transforms.ToTensor()                                  \n","    ])\n","\n","    # Transformation for converting original image array to an image, rotate it randomly between -180 degrees and 180 degrees, and then convert it to a tensor\n","    self.transform4 = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.RandomRotation(180),\n","        transforms.ToTensor()                                \n","    ])\n","\n","    # Transformation for converting original image array to an image, rotate it randomly between -270 degrees and 270 degrees, and then convert it to a tensor\n","    self.transform5 = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.RandomRotation(270),\n","        transforms.ToTensor()                                \n","    ])\n","\n","    # Transformation for converting original image array to an image, rotate it randomly between -300 degrees and 300 degrees, and then convert it to a tensor\n","    self.transform6 = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.RandomRotation(300),\n","        transforms.ToTensor()                               \n","    ])\n","\n","    # Transformation for converting original image array to an image, rotate it randomly between -330 degrees and 330 degrees, and then convert it to a tensor\n","    self.transform7 = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.RandomRotation(330),\n","        transforms.ToTensor()                                 \n","    ])\n","\n","  def __len__(self):\n","    # return length of image samples\n","    return len(self.X)\n","\n","  def __getitem__(self, idx):\n","    # perform transformations on one instance of X\n","    # Original image as a tensor\n","    data = self.transform(self.X[idx])\n","\n","    # Augmented image at 45 degrees as a tensor\n","    aug45 = self.transform1(self.X[idx])\n","\n","    # Augmented image at 90 degrees as a tensor\n","    aug90 = self.transform2(self.X[idx])\n","\n","    # Augmented image at 120 degrees as a tensor\n","    aug120 = self.transform3(self.X[idx])\n","\n","    # Augmented image at 180 degrees as a tensor\n","    aug180 = self.transform4(self.X[idx])\n","\n","    # Augmented image at 270 degrees as a tensor\n","    aug270 = self.transform5(self.X[idx])\n","\n","    # Augmented image at 300 degrees as a tensor\n","    aug300 = self.transform6(self.X[idx])\n","\n","    # Augmented image at 330 degrees as a tensor\n","    aug330 = self.transform7(self.X[idx])      \n","    \n","    # store the transformed images in a list\n","    new_batch = [data, aug45, aug90, aug120, aug180, aug270, aug300, aug330]\n","\n","    # one-hot encode the labels\n","    labels = torch.zeros(4, dtype=torch.float32)\n","    labels[int(self.y[idx])] = 1.0\n","\n","    new_labels = [labels, labels, labels, labels, labels, labels, labels, labels]\n","\n","    # 8 augmented images and corresponding labels per sample will be returned\n","    return (torch.stack(new_labels), torch.stack(new_batch))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zE6mTnC6pQRD"},"source":["## Load the Dataset\n","\n","* Load the **training_data.pickle** file. \n"]},{"cell_type":"code","metadata":{"id":"trcS9hq8q7Dq"},"source":["training_data = pickle.load(open('/content/drive/My Drive/Final Year Project/new_dataset/training_data.pickle', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hY6k--StlVAC"},"source":["Xt = []\n","yt = []\n","features = None\n","labels = None\n","label = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"chU6n1FDHllX"},"source":["Store images in Xt and labels in yt iteratively"]},{"cell_type":"code","metadata":{"id":"9gQx6_RtlwfE"},"source":["for features,labels in training_data:\n","  Xt.append(features)\n","  yt.append(labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VkeZMMj7F6pk","executionInfo":{"status":"ok","timestamp":1604024604883,"user_tz":-330,"elapsed":32525,"user":{"displayName":"Hemant Kumar","photoUrl":"","userId":"05930409951288419175"}},"outputId":"10c77fb6-11e9-4c10-9a48-55afa35a9626","colab":{"base_uri":"https://localhost:8080/"}},"source":["len(yt)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3064"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"QOlU2ZoCpitc"},"source":["## Train Validation Test split\n","\n","Split the dataset for training using cross-validation method.\n","\n","* 70 % of images for training \n","* 15% of images for validating\n","* 15% of images for testing\n"]},{"cell_type":"code","metadata":{"id":"wxT6Znuul5Kq"},"source":["# 70 % training, 15% validating, 15% testing\n","X_train, X_test, y_train, y_test = train_test_split(Xt, yt, test_size=0.3, shuffle=True)  # 70% training, 30% testing\n","X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, shuffle=True)  # split testing set into 50% validation , 50% testing "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"34qYiYRlJoXH"},"source":["Xt = None\n","yt = None\n","features = None\n","labels = None\n","label = None\n","training_data = None "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKlDZh7VqaJj"},"source":["Create training set, validation set and test set using our custom dataset class"]},{"cell_type":"code","metadata":{"id":"rnTKHLR5mCeI"},"source":["train_set = BrainTumorDataset(X_train, y_train)\n","valid_set = BrainTumorDataset(X_valid, y_valid)\n","test_set = BrainTumorDataset(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tfWO3u4YqiUP"},"source":["Print original number of samples in each set"]},{"cell_type":"code","metadata":{"id":"MEL7oQ4Y8NIe","executionInfo":{"status":"ok","timestamp":1604024604889,"user_tz":-330,"elapsed":25658,"user":{"displayName":"Hemant Kumar","photoUrl":"","userId":"05930409951288419175"}},"outputId":"c4dc6654-c817-4a14-f130-002ef61a6c36","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(f\"Number of training samples: {len(X_train)}\")\n","print(f\"Number of validation samples: {len(X_valid)}\")\n","print(f\"Number of testing samples: {len(X_test)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of training samples: 2144\n","Number of validation samples: 460\n","Number of testing samples: 460\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0udsS0Qqql3g"},"source":["Print augmented number of samples in each set"]},{"cell_type":"code","metadata":{"id":"8PJu7hjEi0Ij","executionInfo":{"status":"ok","timestamp":1604024604891,"user_tz":-330,"elapsed":24078,"user":{"displayName":"Hemant Kumar","photoUrl":"","userId":"05930409951288419175"}},"outputId":"9aa49dd6-2849-48f5-aee2-447ee02ea73e","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(f\"Number of augmented training samples: {len(X_train) * 8}\")\n","print(f\"Number of augmented validation samples: {len(X_valid)* 8}\")\n","print(f\"Number of augmented testing samples: {len(X_test)* 8}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of augmented training samples: 17152\n","Number of augmented validation samples: 3680\n","Number of augmented testing samples: 3680\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S9XaKzTZqpW0"},"source":["Create a DataLoader for each set with batch size of 4 and shuffling enabled"]},{"cell_type":"code","metadata":{"id":"-5Eil06jrRUT"},"source":["train_gen = DataLoader(train_set, batch_size=4, shuffle=True, pin_memory=True, num_workers=8)\n","valid_gen = DataLoader(valid_set, batch_size=4, shuffle=True, pin_memory=True, num_workers=8)\n","test_gen = DataLoader(test_set, batch_size=10, shuffle=True, pin_memory=True, num_workers=8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VE1aH9bJq1HD"},"source":["Get device to set the training to run on GPU or CPU later based on its availibility"]},{"cell_type":"code","metadata":{"id":"ka_a8OAPtNHY"},"source":["device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","device = torch.device(device_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eHncA4uwrAnB"},"source":["## Build the Model\n","\n"]},{"cell_type":"code","metadata":{"id":"Lq3_lCEttAJk","executionInfo":{"status":"ok","timestamp":1604024623002,"user_tz":-330,"elapsed":37757,"user":{"displayName":"Hemant Kumar","photoUrl":"","userId":"05930409951288419175"}},"outputId":"73948296-163f-4889-ea0a-cbb983ebe436","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["561d2064430c4da9bd3c1ab7a40ff1ae","f5876312446a4c3f8eacc983c26951e0","67d278f9e50c47b4aae7c2ad19353577","03db0d2335ab461daa38bb624df06cd3","906e832f95454f93ac43f0893aa8acbc","c2b86978266d4625bc8fd6e30db2e661","be9f0d7efc6245579aa7055a45bc451c","987b15ca0c5a49cc9939ea915f4ba62d"]}},"source":["# instantiate transfer learning model\n","resnet_model = models.resnet50(pretrained=True)\n","\n","# set all paramters as trainable\n","for param in resnet_model.parameters():\n","    param.requires_grad = True\n","\n","# get input of fc layer\n","n_inputs = resnet_model.fc.in_features\n","\n","# redefine fc layer / top layer/ head for our classification problem\n","resnet_model.fc = nn.Sequential(nn.Linear(n_inputs, 64),\n","                                nn.SELU(),\n","                                nn.Dropout(p=0.4),\n","                                nn.Linear(64, 64),\n","                                nn.SELU(),\n","                                nn.Dropout(p=0.4),\n","                                nn.Linear(64, 4),\n","                                nn.LogSigmoid())\n","\n","# set all paramters of the model as trainable\n","for name, child in resnet_model.named_children():\n","  for name2, params in child.named_parameters():\n","    params.requires_grad = True\n","\n","# set model to run on GPU or CPU absed on availibility\n","resnet_model.to(device)\n","\n","# print the trasnfer learning NN model's architecture\n","resnet_model"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"561d2064430c4da9bd3c1ab7a40ff1ae","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (4): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (5): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Sequential(\n","    (0): Linear(in_features=2048, out_features=64, bias=True)\n","    (1): SELU()\n","    (2): Dropout(p=0.4, inplace=False)\n","    (3): Linear(in_features=64, out_features=64, bias=True)\n","    (4): SELU()\n","    (5): Dropout(p=0.4, inplace=False)\n","    (6): Linear(in_features=64, out_features=4, bias=True)\n","    (7): LogSigmoid()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"GyICmnEqV-i2"},"source":["torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_QdrEPESsKQi"},"source":["## Set Training Configuration"]},{"cell_type":"code","metadata":{"id":"cokrpXp3ud8_"},"source":["# loss function\n","# if GPU is available set loss function to use GPU\n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","# optimizer\n","optimizer = torch.optim.SGD(resnet_model.parameters(), momentum=0.9, lr=3e-4)\n","\n","# number of training iterations\n","epochs = 30\n","\n","# empty lists to store losses and accuracies\n","train_losses = []\n","test_losses = []\n","train_correct = []\n","test_correct = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7E_rg4eJs7Xq"},"source":["## Util function\n","\n","###### Checkpoint Saver\n"]},{"cell_type":"code","metadata":{"id":"MnUksQpFNxNK"},"source":["def save_checkpoint(state, is_best, filename='/content/drive/My Drive/Final Year Project/bt_resnet50_ckpt_v2.pth.tar'):\n","    torch.save(state, filename)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ci1Lw3UgtU13"},"source":["## Train the model"]},{"cell_type":"code","metadata":{"id":"0TWAlOEau6k7"},"source":["# set training start time\n","start_time = time.time()\n","\n","\n","\n","# set best_prec loss value as 2 for checkpoint threshold\n","best_prec1 = 2\n","\n","# empty batch variables\n","b = None\n","train_b = None\n","test_b = None\n","\n","# start training\n","for i in range(epochs):\n","    # empty training correct and test correct counter as 0 during every iteration\n","    trn_corr = 0\n","    tst_corr = 0\n","    \n","    # set epoch's starting time\n","    e_start = time.time()\n","    \n","    # train in batches\n","    for b, (y, X) in enumerate(train_gen):\n","        # set label as cuda if device is cuda\n","\n","        X = X.to(device)\n","        y= y.to(device)\n","          \n","          # forward pass image sample\n","        y_pred = resnet_model(X.view(-1, 3, 512, 512))\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","          # calculate loss\n","        loss = criterion(y_pred.float(), torch.argmax(y.view(32, 4), dim=1).long())\n","\n","          # get argmax of predicted tensor, which is our label\n","        predicted = torch.argmax(y_pred, dim=1).data\n","          # if predicted label is correct as true label, calculate the sum for samples\n","        batch_corr = (predicted == torch.argmax(y.view(32, 4), dim=1)).sum()\n","          # increment train correct with correcly predicted labels per batch\n","        trn_corr += batch_corr\n","          \n","          # set optimizer gradients to zero\n","        optimizer.zero_grad()\n","          # back propagate with loss\n","        loss.backward()\n","          # perform optimizer step\n","        optimizer.step()\n","      \n","\n","    # set epoch's end time\n","    e_end = time.time()\n","    # print training metrics\n","    print(f'Epoch {(i+1)} Batch {(b+1)*4}\\nAccuracy: {trn_corr.item()*100/(4*8*b):2.2f} %  Loss: {loss.item():2.4f}  Duration: {((e_end-e_start)/60):.2f} minutes') # 4 images per batch * 8 augmentations per image * batch length\n","\n","    # some metrics storage for visualization\n","    train_b = b\n","    train_losses.append(loss)\n","    train_correct.append(trn_corr)\n","\n","    X, y = None, None\n","\n","    # validate using validation generator\n","    # do not perform any gradient updates while validation\n","    with torch.no_grad():\n","        for b, (y, X) in enumerate(valid_gen):\n","            X, y = X.to(device), y.to(device)\n","\n","                # forward pass image\n","            y_val = resnet_model(X.view(-1, 3, 512, 512))\n","\n","                # get argmax of predicted tensor, which is our label\n","            predicted = torch.argmax(y_val, dim=1).data\n","\n","                # increment test correct with correcly predicted labels per batch\n","            tst_corr += (predicted == torch.argmax(y.view(32, 4), dim=1)).sum()\n","\n","    # get loss of validation set\n","    loss = criterion(y_val.float(), torch.argmax(y.view(32, 4), dim=1).long())\n","    # print validation metrics\n","    print(f'Validation Accuracy {tst_corr.item()*100/(4*8*b):2.2f} Validation Loss: {loss.item():2.4f}\\n')\n","\n","    # if current validation loss is less than previous iteration's validatin loss create and save a checkpoint\n","    is_best = loss < best_prec1\n","    best_prec1 = min(loss, best_prec1)\n","    save_checkpoint({\n","            'epoch': i + 1,\n","            'state_dict': resnet_model.state_dict(),\n","            'best_prec1': best_prec1,\n","        }, is_best)\n","\n","    # some metrics storage for visualization\n","    test_b  = b\n","    test_losses.append(loss)\n","    test_correct.append(tst_corr)\n","\n","# set total training's end time\n","end_time = time.time() - start_time    \n","\n","# print training summary\n","print(\"\\nTraining Duration {:.2f} minutes\".format(end_time/60))\n","print(\"GPU memory used : {} kb\".format(torch.cuda.memory_allocated()))\n","print(\"GPU memory cached : {} kb\".format(torch.cuda.memory_cached()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U5hiXkZkwW-W"},"source":["torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Rzl2f2kteb8"},"source":["## Save the model\n","\n","Save the model after the training is completed"]},{"cell_type":"code","metadata":{"id":"K4yzcJMmwPnY"},"source":["torch.save(resnet_model.state_dict(), '/content/drive/My Drive/Final Year Project/bt_resnet50_model.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q-2R8Yputlvh"},"source":["## Evaluation"]},{"cell_type":"markdown","metadata":{"id":"hPEBr7E1tobU"},"source":["Print the validation accuracy of the model calculated using validation set during training "]},{"cell_type":"code","metadata":{"id":"4w0EuNLQvmNV"},"source":["print(f'Validation accuracy: {test_correct[-1].item()*100/(test_b*8*4):.2f}%')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EnauSi8Dt04I"},"source":["Plot the loss graph"]},{"cell_type":"code","metadata":{"id":"TttSopdFvtr0"},"source":["plt.plot(train_losses, label='Training loss')\n","plt.plot(test_losses, label='Validation loss')\n","plt.title('Loss Metrics')\n","plt.ylabel('Loss')\n","plt.xlabel('Epochs')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YKzECNFZvx9u"},"source":["plt.plot([t*(1/171) for t in train_correct], label='Training accuracy')\n","plt.plot([t*(1/36) for t in test_correct], label='Validation accuracy')\n","plt.title('Accuracy Metrics')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epochs')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SsVJuIwych21"},"source":["resnet_model.load_state_dict(torch.load('/content/drive/My Drive/Final Year Project/bt_resnet50_model.pt'))\n","train_gen = None\n","valid_gen = None\n","train_set = None\n","valid_set = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LfVhP2dmvdoi"},"source":["Set model to evaluation mode\n","\n","Calculate loss, correctly classified samples, predicted values, labels and store them in a list using test dataloader"]},{"cell_type":"code","metadata":{"id":"WZJ7w9ztv1pb","executionInfo":{"status":"ok","timestamp":1604024806588,"user_tz":-330,"elapsed":62834,"user":{"displayName":"Hemant Kumar","photoUrl":"","userId":"05930409951288419175"}},"outputId":"3a37288e-0d38-484d-9bf9-e43415eaca2d","colab":{"base_uri":"https://localhost:8080/"}},"source":["# set model to evaluation mode\n","resnet_model.eval()\n","\n","# perform no gradient updates\n","with torch.no_grad():\n","    # soem metrics storage for visualization and analysis\n","    correct = 0\n","    test_loss = []\n","    test_corr = []\n","    labels = []\n","    pred = []\n","    # perform test set evaluation batch wise\n","    for (y, X) in test_gen:\n","        # set label to use CUDA if available\n","        X, y = X.to(device), y.to(device)\n","\n","        # append original labels\n","        labels.append(torch.argmax(y.view(10 * 8, 4), dim=1).data)\n","\n","        # perform forward pass\n","        y_val = resnet_model(X.view(-1, 3, 512, 512))\n","\n","        # get argmax of predicted values, which is our label\n","        predicted = torch.argmax(y_val, dim=1).data\n","        # append predicted label\n","        pred.append(predicted)\n","\n","        # calculate loss\n","        loss = criterion(y_val.float(), torch.argmax(y.view(10 * 8, 4), dim=1).long())\n","\n","        # increment correct with correcly predicted labels per batch\n","        correct += (predicted == torch.argmax(y.view(10 * 8, 4), dim=1)).sum()\n","\n","        # append correct samples labels and losses\n","        test_corr.append(correct)\n","        test_loss.append(loss)\n","        \n","print(f\"Test Loss: {test_loss[-1].item():.4f}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test Loss: 0.0003\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AqDm19ZSvyY2"},"source":["Print the test accuracy\n"]},{"cell_type":"code","metadata":{"id":"HJm2yjzHbLlP","executionInfo":{"status":"ok","timestamp":1604024816427,"user_tz":-330,"elapsed":1432,"user":{"displayName":"Hemant Kumar","photoUrl":"","userId":"05930409951288419175"}},"outputId":"c6cbc9d1-6ca7-43c3-c405-0a5bae2a7286","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(f'Test accuracy: {test_corr[-1].item()*100/(460*8):.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test accuracy: 99.73%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T6fxSb68v0uf"},"source":["Convert list of tensors to tensors"]},{"cell_type":"code","metadata":{"id":"jDRv0IhThKhP"},"source":["labels = torch.stack(labels)\n","pred = torch.stack(pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ps2QnU4ov4Sr"},"source":["Define ground-truth labels as a list"]},{"cell_type":"code","metadata":{"id":"7BsASCMlL2_z"},"source":["LABELS = ['Meningioma', 'Glioma', 'Pitutary']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wubHPKqov61w"},"source":["Plot the confusion matrix"]},{"cell_type":"code","metadata":{"id":"3cfoG8Q0gMKZ","executionInfo":{"status":"ok","timestamp":1604025074945,"user_tz":-330,"elapsed":23521,"user":{"displayName":"Hemant Kumar","photoUrl":"","userId":"05930409951288419175"}},"outputId":"30912ce7-d3fc-466b-9fdd-4cf09f0f85a0","colab":{"base_uri":"https://localhost:8080/","height":388}},"source":["arr = confusion_matrix(pred.view(-1).cpu(), labels.view(-1).cpu())\n","df_cm = pd.DataFrame(arr, LABELS, LABELS)\n","plt.figure(figsize = (9,6))\n","sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap='viridis')\n","plt.xlabel(\"Prediction\")\n","plt.ylabel(\"Target\")\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 648x432 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}